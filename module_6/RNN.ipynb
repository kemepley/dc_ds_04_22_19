{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:brown;\">  Recurrent neural nets</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Looping network](./img/RNN_colah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNNs can produce amazing results <a href =\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">blog</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson plan \n",
    "1. Why classic neural nets are not enough?\n",
    "2. Word embeddings - word2vec\n",
    "3. Categorical embeddings\n",
    "4. RNN \n",
    "5. Takeaways\n",
    "6. Hands on word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic nets vs. RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic:\n",
    "    - Inputs and outputs must be fixed-sized vectors\n",
    "    - No idea of location or time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/diags.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Which word produces the highest probability to be next given we have seen n specific other words before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words: Thank, you, Hello, goodbye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we have 4 words and we are looking at 2-gram? \n",
    "    Example: no. of times Thank you occurs divided by number of times Thank occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the probabilty of \n",
    " - Thank Hello\n",
    " - Thank you\n",
    " - Thank goodbye\n",
    " - Thank Thank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we needed to do 4 calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_calc_to_do(gram, voc_size):\n",
    "    '''This function needs to calculate all combos \n",
    "    of words'''\n",
    "    \n",
    "    return np.prod(np.repeat(voc_size, gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4477988020393345024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "how_many_calc_to_do(7, 10000)\n",
    "# Notice that this is only an approxiamtion and it can be implemented in more efficient ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/one_hot_encoding_distance_on_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight I: \n",
    "    we can actually just turn each word to a random vector sized 100/200/300, \n",
    "    train a classic neural net to predict the next word and update both the weights and the random vectors.\n",
    "    You can think of it as just another layer of weights multiplying the one hot encoded inputs.\n",
    "<a href=\"http://hunterheidenreich.com/blog/intro-to-word-embeddings/\">word_embed blog</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\">word_embed blog II</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![W2V](./img/w2v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective: maximize the sum of probabilities of each word given its observed window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea is very strong in comparison to other options: \n",
    "\n",
    "    Bag of words - just count occourences \n",
    "    TF-IDF - word is either informative or not but has no relation to other words\n",
    "    one-hot encoder: for the computer paris-france is the same distance as paris-blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distance and direction are meaningful! King - man = Queen - woman\n",
    "- Now the words massive and huge are similar!\n",
    "- Extends to sentences, paragraphs and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/see_attached_word_embed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have reduced the dimension of the vocublary by a big factor!\n",
    "example: from 80,000 to 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.lemay.ai/demo/wordEmbedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insight I.I  The same thing can be applied to any categorical variable. \n",
    "##### With enough training data we can learn its continous position in space - state of the art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![categorical_embed](./img/categorical_embedding.png) # image I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![german_states](./img/german_states_mapped_2D.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight II: \n",
    "        well, even if we can include many words (large n-gram), how can we capture context?\n",
    "        If the text mentioned queen Mary and few pages later is talking about the queen, how will our network \n",
    "        know her name is Mary? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea I: Memory - your current choices are based on previous understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some cell in the network to keep previous memory and combine with current input to predict next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/memory_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem: calculating the derivative (aka gradient) is problematic, either infinite or zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the memory at time t is the memory at time t-1 times a weight vector:\n",
    "    $h_t = W*h_{t-1}$\n",
    "Then:\n",
    "    $h_t = W^t * h_0$ \n",
    "    \n",
    "  $W > 1$ $h_t --> \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: LSTM/GRU\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">LSTM/GRU blog</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/RNNs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/LSTM_colah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea II: gates: don't multiply, use addition for memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - cell state\n",
    "    - candidates  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gates\n",
    "- forget - information to throw (0 means throw all from the cell state)\n",
    "- input - what values we are going to update\n",
    "- output - filter which values of the cell we are going to output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current cell state is the sum of forgetting and updating with new candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=SysgYptB198\">Intuition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  - Translate part by part\n",
    "###### -  Use attention weights - how much attention should you give to each word in the input (update weights to each new word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word embeddings\n",
    "- Word/categorical embeddings gives meaning to words in relation to one another\n",
    "- Word/categorical embeddings are computationally efficient\n",
    "- Training is done through a classic NN with small window around words\n",
    "\n",
    "##### RNN\n",
    "- Old generation RNNs suffered from exploding/vanishing gradients\n",
    "- New generation RNNs (commonly LSTM or GRU) are using memory gates to mitigate this problem\n",
    "- RNNs are just multiple copies of a NN connected by the hidden layer\n",
    "- Training is done again by backpropogation\n",
    "- Weights are shared accros all network\n",
    "- RNN's can be used for any sequence. Unlike time series models can include both time and features.\n",
    "- Are flexible in input and output sizes\n",
    "- Amazing results in NLP, recommendations and many more.\n",
    "- Many flavours - BRNN, CRNN...\n",
    "\n",
    "##### Attention\n",
    "- Typicall for translations/images\n",
    "- Weight all the words in one language to decide how much they should influence input to translated language\n",
    "- components: word weights, BRNN, RNN, context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on Word2Vec/word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/OmersGuest/Documents/dc_ds_04_22_19/module_6/JEOPARDY_QUESTIONS1.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216930"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'HISTORY',\n",
       " 'air_date': '2004-12-31',\n",
       " 'question': \"'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\",\n",
       " 'value': '$200',\n",
       " 'answer': 'Copernicus',\n",
       " 'round': 'Jeopardy!',\n",
       " 'show_number': '4680'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first element in our list\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec requires that our text have the form of a list\n",
    "# of 'sentences', where each sentence is itself a list of\n",
    "# words. How can we put our _Jeopardy!_ clues in that shape?\n",
    "\n",
    "text = []\n",
    "\n",
    "for clue in data:\n",
    "    sentence = clue['question'].translate(str.maketrans('', '',\n",
    "                                                        string.punctuation)).split(' ')\n",
    "    \n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        new_sent.append(word.lower())\n",
    "    \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " 'mans',\n",
       " 'theory']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the new structure of our first clue\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply a matter of\n",
    "# instantiating a Word2Vec object.\n",
    "model = gensim.models.Word2Vec(text, sg=1)\n",
    "## sg means skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11336570, 15849970)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train, call 'train()'!\n",
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a2fbd94a8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '.wv' attribute stores the word vectors\n",
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2660608 ,  0.74738264,  0.6810505 , -0.47299108,  0.05217691,\n",
       "       -0.10152421, -0.20832658,  0.08030565,  0.32037446,  0.3153305 ,\n",
       "       -0.34866828, -0.31779078,  0.22926913,  0.60894746, -0.56666803,\n",
       "        0.01631304, -0.31678593, -0.20412245,  0.27447847, -0.67274284,\n",
       "        0.36620042, -0.23380297,  0.13526577,  0.2926762 ,  0.18137805,\n",
       "       -0.2558905 ,  0.23138794,  0.04789907,  0.21861915,  0.24400485,\n",
       "       -0.3116787 ,  0.47753632, -0.14129499,  0.03673249,  0.19247477,\n",
       "        0.26503217, -0.09043661,  0.14087467, -0.3829878 ,  0.29696804,\n",
       "       -0.05752321,  0.8588681 ,  0.14684854,  0.38990337,  0.09335101,\n",
       "       -0.40334013,  0.0594354 ,  0.12365463, -0.0052815 ,  0.30034325,\n",
       "       -0.25350818,  0.48246992,  0.12052973,  0.08013797,  0.07505435,\n",
       "        0.45640057, -0.6654021 , -0.44835666, -0.00786007, -0.5183773 ,\n",
       "       -0.00484134, -0.01148336,  0.28142917, -0.01758763,  0.06659041,\n",
       "        0.94368154, -0.16261122, -0.15630189,  0.30726194,  0.19767079,\n",
       "        0.0752794 ,  0.24242386,  0.3026063 ,  0.3059553 , -0.17773063,\n",
       "       -0.34614375,  0.21932346, -0.15979928, -0.34958056,  0.7385357 ,\n",
       "        0.49426782,  0.19162491,  0.17557271,  0.09977435,  0.03595296,\n",
       "        0.00397836, -0.4142427 ,  0.28141183,  0.35480532, -0.05529991,\n",
       "       -0.11867512, -0.04037756,  0.0313993 ,  0.11239044,  0.08555908,\n",
       "        0.04677899, -0.28239715, -0.35449657,  0.101408  , -0.05059252],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['child']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model.wv methods\n",
    "#### 'most_similar()' and 'similarity()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('despair', 0.7301496863365173),\n",
       " ('wherefore', 0.7294198274612427),\n",
       " ('virtue', 0.7269254922866821),\n",
       " ('jealousy', 0.7168462872505188),\n",
       " ('shakespearebr', 0.7144066095352173),\n",
       " ('arise', 0.7120428681373596),\n",
       " ('ignorance', 0.7045302391052246),\n",
       " ('shame', 0.703694224357605),\n",
       " ('kindness', 0.7031581401824951),\n",
       " ('psalm', 0.6967296004295349)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artwork', 0.6930415034294128),\n",
       " ('paints', 0.6927978992462158),\n",
       " ('chippendale', 0.6907036304473877),\n",
       " ('fastener', 0.6883490085601807),\n",
       " ('pottery', 0.6808674335479736),\n",
       " ('bicycles', 0.6789553165435791),\n",
       " ('ceramic', 0.6788221001625061),\n",
       " ('flooring', 0.6764292120933533),\n",
       " ('cabriole', 0.675460934638977),\n",
       " ('monogram', 0.6749094724655151)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6630453"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('furniture', 'jewelry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carnivore', 0.8128958344459534),\n",
       " ('cheetah', 0.7955524921417236),\n",
       " ('scavenger', 0.7922323346138),\n",
       " ('rodent', 0.7912538051605225),\n",
       " ('arachnid', 0.7891545295715332),\n",
       " ('parrot', 0.7882996797561646),\n",
       " ('shorthaired', 0.7842555642127991),\n",
       " ('wading', 0.7803277373313904),\n",
       " ('flightless', 0.7746297121047974),\n",
       " ('shorttailed', 0.7745615839958191)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal', 'pet', 'mammal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lizard', 0.3776397705078125),\n",
       " ('insect', 0.3743097484111786),\n",
       " ('breed', 0.36435168981552124),\n",
       " ('camel', 0.35821762681007385),\n",
       " ('marsupial', 0.35507845878601074),\n",
       " ('domesticated', 0.3544871509075165),\n",
       " ('rodent', 0.3539559841156006),\n",
       " ('extinction', 0.3437584340572357),\n",
       " ('monkey', 0.3437137007713318),\n",
       " ('sheep', 0.34092289209365845)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal'], negative='pet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throne', 0.297690749168396),\n",
       " ('monarch', 0.27509433031082153),\n",
       " ('queen', 0.2577328681945801)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative='man', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fargo', 0.6399749517440796),\n",
       " ('tyra', 0.617228090763092),\n",
       " ('npr', 0.6081739664077759),\n",
       " ('retitled', 0.5980387330055237),\n",
       " ('dogpatch', 0.5920142531394958),\n",
       " ('pageant', 0.5874358415603638),\n",
       " ('wichita', 0.5813845992088318),\n",
       " ('bombay', 0.5791819095611572),\n",
       " ('computergenerated', 0.5788818597793579),\n",
       " ('crossroads', 0.5769336223602295)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zambia', 0.692581057548523),\n",
       " ('marianas', 0.6910848617553711),\n",
       " ('commonwealth', 0.6846738457679749),\n",
       " ('territories', 0.6586534976959229),\n",
       " ('uruguay', 0.6584171652793884),\n",
       " ('albania', 0.646291196346283),\n",
       " ('manitoba', 0.6458621621131897),\n",
       " ('yalta', 0.6444039940834045),\n",
       " ('chile', 0.6434510946273804),\n",
       " ('liechtenstein', 0.6419185996055603)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shakespeares', 0.7422709465026855),\n",
       " ('sophocles', 0.7291842103004456),\n",
       " ('euripides', 0.7068688869476318),\n",
       " ('romeo', 0.6914283037185669),\n",
       " ('ibsen', 0.6874734163284302),\n",
       " ('shakespearean', 0.6804429292678833),\n",
       " ('shaws', 0.6732524633407593),\n",
       " ('falstaff', 0.6704421043395996),\n",
       " ('hamlet', 0.6690429449081421),\n",
       " ('moliere', 0.6675474643707275)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emperors', 0.2684832811355591),\n",
       " ('dictator', 0.25363075733184814),\n",
       " ('ussr', 0.21678945422172546),\n",
       " ('regime', 0.2130197286605835),\n",
       " ('emperor', 0.19828079640865326),\n",
       " ('imprisoned', 0.1896682232618332),\n",
       " ('presidents', 0.18698829412460327),\n",
       " ('headlines', 0.18627069890499115),\n",
       " ('fascist', 0.18320727348327637),\n",
       " ('invaded', 0.18310856819152832)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president', 'germany'], negative='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 'doesnt_match()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['breakfast', 'lunch', 'frog', 'food'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
